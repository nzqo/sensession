{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b97475-2556-41d3-9778-b00547a7e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b1193-9ffa-4a2a-b8f0-a4f96ee08627",
   "metadata": {},
   "source": [
    "# Generic Helpers\n",
    "\n",
    "Some generic helpers to simplify the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64dedf-4aba-4689-87d1-18ababbd003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_unwrapped(df: pl.DataFrame, func, unwrap_cols: list) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an operation on a temporarily unwrapped dataframe.\n",
    "\n",
    "    Args:\n",
    "        df  : DataFrame to work on\n",
    "        func : func to apply to the unwrapped dataframe\n",
    "        unwrap_cols : Column names to unwrap\n",
    "\n",
    "    Returns:\n",
    "        A rewrapped dataframe after application of func on the unwrapped one.\n",
    "    \"\"\"\n",
    "    df = df.with_row_index(\"remap_index\")\n",
    "    unwrap_df = df.explode(*unwrap_cols)\n",
    "    unwrap_df = func(unwrap_df)\n",
    "    rewrap_df = unwrap_df.group_by(\"remap_index\").agg(*unwrap_cols)\n",
    "    return df.update(rewrap_df, on=\"remap_index\").drop(\"remap_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4854f949-fde2-4fbc-9544-b4fb3a63f533",
   "metadata": {},
   "source": [
    "# Aligning Sequence Numbers\n",
    "\n",
    "Our data consists of multiple receivers collecting CSI harvested from the exact same signal, split from one antenna using an RF-waveguide-splitter. Not always do the cards see the same thing. For a direct 1-1 comparison between values, we might only want to consider data points that all devices have caught up.\n",
    "\n",
    "Unmodified frames are sent with increasing sequence numbers, which serve as unique identifier of frames. Hence, by comparing sequence numbers, we can figure out which packages were indeed used by all receivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd8237-37b5-4a7e-990c-bcbb04dc6812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_sequence_nums(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Sequence number sequentially enumerates the packets sent to invoke CSI collection.\n",
    "    To align sequence numbers, we discard all CSI values in which at least one card\n",
    "    has failed the capture.\n",
    "    \"\"\"\n",
    "    logger.info(\"Aligning sequence numbers ...\")\n",
    "\n",
    "    # Only keep captures where all cards successfully saw CSI\n",
    "    df = (\n",
    "        df.group_by(\"session_id\", maintain_order=True)\n",
    "        .agg(pl.col(\"receiver_name\").n_unique().alias(\"n_receivers\"))\n",
    "        .join(df, on=\"session_id\")\n",
    "    )\n",
    "\n",
    "    filtered_sequences = (\n",
    "        df.group_by(\"sequence_number\", \"session_id\", maintain_order=True)\n",
    "        .agg(\n",
    "            pl.count(\"receiver_name\").alias(\"sns_count\"), pl.col(\"n_receivers\").first()\n",
    "        )\n",
    "        .filter(pl.col(\"sns_count\") == pl.col(\"n_receivers\"))\n",
    "    )\n",
    "\n",
    "    return df.join(filtered_sequences, on=[\"sequence_number\", \"session_id\"], how=\"semi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415da936-0c03-4088-9fce-84a194f80175",
   "metadata": {},
   "source": [
    "# Discard non-precoded frames\n",
    "\n",
    "We precode frames to emulate channel changes. To have a baseline, we often also use \"training\" or \"warmup\" runs, in which we do send the unmodified frame. For some postprocessing, these frames are irrelevant and can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc0677-0115-4793-9129-51dc313a58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_unmodified(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Before sending curve precoded frames, we send a \"warmup\" sequence of unmodified\n",
    "    frames. This functions discards those from the dataframe.\n",
    "    \"\"\"\n",
    "    logger.info(\"Discarding unmodified frame captures ...\")\n",
    "    # Some of our captures are taken without any movement or obstructions in between.\n",
    "    # These can possibly be used later to normalize e.g. scalings. However, for the\n",
    "    # visualization at hand, we do not need them.\n",
    "    UNMODIFIED_MASK_NAME: str = \"_unmodified\"\n",
    "    return df.filter(pl.col(\"mask_name\") != UNMODIFIED_MASK_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0df726-9cca-4726-b241-5f045e701a12",
   "metadata": {},
   "source": [
    "# Merging session pairs\n",
    "\n",
    "One issue in our setup is that two of the cards are incompatible (iwl5300 and qca). More precisely, the frames which the corresponding tools require to yield CSI are incompatible. Hence, we can not collect CSI from these cards concurrently.\n",
    "\n",
    "In our experiments, we run sessions including either of those cards back-to-back.\n",
    "\n",
    "Since the remaining cards can always capture (and we make use of that), this creates an imbalance in the amount of data available per receiver. In session pair merging, we try to \"merge\" the data from two back-to-back sessions, pretending as if they were from one.\n",
    "\n",
    "**NOTE**: This only works as long as no session is missing. Sessions have identifiers but are not enumerated, so when one of the pair sessions is missing, we do not detect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeab8fa-9e2c-4f01-8319-ca582303d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_session_pairs(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    While qca and iwl5300 are incompatible, ax210 and asus can always collect CSI.\n",
    "    We perform back-to-back sessions, one for iwl and one for qca, with the other\n",
    "    cards always capturing. This creates a data imbalance.\n",
    "\n",
    "    This function merges all pairs of such back-to-back sessions, taking only the\n",
    "    \"new\" values from the second pair.\n",
    "\n",
    "    **NOTE**: This assumes that there are actually pairs in the data!\n",
    "    If any of the sessions failed, this order is broken and the function invalid.\n",
    "    \"\"\"\n",
    "    logger.info(\"Balancing data by merging pairs of captures...\")\n",
    "    session_pair_idx = (\n",
    "        df.select(\"session_id\")\n",
    "        .unique(\"session_id\", maintain_order=True)\n",
    "        .with_row_index(\"num_sess_pair\")\n",
    "        .with_columns(pl.col(\"num_sess_pair\") // 2)\n",
    "    )\n",
    "    df = df.join(session_pair_idx, on=\"session_id\")\n",
    "\n",
    "    # Function to take groups with session pairs and merge them!\n",
    "    def session_merger(group):\n",
    "        unique_ids = group.unique(\"session_id\", maintain_order=True)\n",
    "        assert unique_ids.shape[0] == 2, \"Session merger must be fed pairs of sessions!\"\n",
    "        id_a = unique_ids.item(0, \"session_id\")\n",
    "        id_b = unique_ids.item(1, \"session_id\")\n",
    "\n",
    "        # Take receivers from session and append data captured by qca in following session\n",
    "        new_df = pl.concat(\n",
    "            [\n",
    "                group.filter(pl.col(\"session_id\") == id_a),\n",
    "                group.filter(pl.col(\"session_id\") == id_b).filter(\n",
    "                    pl.col(\"receiver_name\") == \"qca\"\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return new_df\n",
    "\n",
    "    return (\n",
    "        df.group_by(\"num_sess_pair\", maintain_order=True)\n",
    "        .map_groups(session_merger)\n",
    "        .drop(\"num_sess_pair\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62676e40-b146-43fb-8a83-306527e5c14e",
   "metadata": {},
   "source": [
    "# Discarding low counts\n",
    "\n",
    "Sometimes the cards perform subpar. When cards drop lots of frames, the data is expected to look pretty bad, especially in terms of temporal coherence. It is thus desirable to drop it usually. \n",
    "\n",
    "In some experiments, e.g. the curve experiment, we also want to drop data if, possibly as a result of dropping low session counts, not enough sessions are present to represent a precoding curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d69f70a-9230-430b-ab3a-ef44234e8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_low_counts(df: pl.DataFrame, min_count: int = 600, grouped: bool = False):\n",
    "    \"\"\"\n",
    "    Discard all sessions in which the number of CSI captured is below threshold.\n",
    "\n",
    "    Args:\n",
    "        min_count : The threshold minimum count of CSI values\n",
    "        grouped   : Whether to discard in neighboring pairs of two. Should be done only\n",
    "            when groups are present and haven't been merged.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Discarding sessions with low  (<{min_count}) CSI capture counts ...\")\n",
    "\n",
    "    # Assumes sequence numbers were aligned; This way we can find the captured\n",
    "    # frame counts per session by considering unique SNs per session\n",
    "    sess_counts = (\n",
    "        df.unique(\n",
    "            [\"session_id\", \"sequence_number\", \"receiver_name\"], maintain_order=True\n",
    "        )\n",
    "        .group_by(\"session_id\", \"receiver_name\", maintain_order=True)\n",
    "        .len()\n",
    "    )\n",
    "\n",
    "    # We discard in groups of two, since we capture in groups of two (iwl and qca)\n",
    "    to_discard = sess_counts.filter(pl.col(\"len\") < min_count)\n",
    "\n",
    "    if grouped:\n",
    "        # Enumerate the sessions\n",
    "        to_discard = to_discard.join(\n",
    "            to_discard.select(\"session_id\")\n",
    "            .unique(\"session_id\", maintain_order=True)\n",
    "            .with_row_index(\"session_idx\"),\n",
    "            on=\"session_id\",\n",
    "        )\n",
    "\n",
    "        # Find even and odd sessions that are to be discarded\n",
    "        even_discards = to_discard.filter(pl.col(\"session_idx\") % 2 == 0)\n",
    "        odd_discards = to_discard.filter(pl.col(\"session_idx\") % 2 == 1)\n",
    "\n",
    "        # Merge with the correspondingly paired sessions\n",
    "        to_discard = pl.concat(\n",
    "            [\n",
    "                even_discards,\n",
    "                odd_discards,\n",
    "                even_discards.with_columns(session_idx=pl.col(\"session_idx\") + 1),\n",
    "                odd_discards.with_columns(session_idx=pl.col(\"session_idx\") - 1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    logger.info(\n",
    "        \"Discarding sessions with insufficient capture rates : \\n\"\n",
    "        + f\"{to_discard.get_column('session_id').to_list()}\"\n",
    "    )\n",
    "\n",
    "    df = df.join(to_discard, on=\"session_id\", how=\"anti\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def discard_low_session_counts(df: pl.DataFrame, min_num_sessions: int = 10):\n",
    "    \"\"\"\n",
    "    In our experiments, we use curves to precode WiFi frames. Transmission of one curve\n",
    "    precoded frame sequence is a session. For each curve, we perform multiple sessions.\n",
    "\n",
    "    This function discards curves completely if an insufficient amount of sessions is\n",
    "    present in the dataframe.\n",
    "\n",
    "    Args:\n",
    "        min_num_sessions: Minimum number of sessions to ensure are present for each\n",
    "            curve.\n",
    "    \"\"\"\n",
    "    logger.info(\n",
    "        f\"Discarding data for curves with less than {min_num_sessions} sessions ...\"\n",
    "    )\n",
    "\n",
    "    curves_to_remove = (\n",
    "        df.unique([\"session_id\", \"num_curve\", \"receiver_name\"], maintain_order=True)\n",
    "        .group_by(\"num_curve\", \"receiver_name\", maintain_order=True)\n",
    "        .len()\n",
    "        .filter(pl.col(\"len\") < min_num_sessions)\n",
    "        .drop(\"len\")\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        \"Discarding curves: \\n\"\n",
    "        + f\"{curves_to_remove.get_column('num_curve').to_list()}\"\n",
    "    )\n",
    "    return df.join(curves_to_remove, on=\"num_curve\", how=\"anti\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014b890-43c7-474c-ae3e-3b0cddb8ac1b",
   "metadata": {},
   "source": [
    "# Removing Edge subcarriers\n",
    "\n",
    "In Nexmon, specifically the rt-ac86u asus receiver, one of the subcarriers is broken, i.e. never yields sensible data. To still work with a balance dataset of equal shape, we may want to discard the corresponding edge subcarriers (28 and -28) for all receivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2accd2a2-b171-4f53-99b7-877b087a45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_edge_subcs(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    logger.info(\n",
    "        \"Removing edge subcarriers for aligned data (asus has a broken one) ...\"\n",
    "    )\n",
    "    # Unwrap, remove edge subcarrier, rewrap\n",
    "    filtered = (\n",
    "        df.with_row_index()\n",
    "        .explode(\"subcarrier_idxs\", \"csi_abs\", \"csi_phase\")\n",
    "        .filter((pl.col(\"subcarrier_idxs\") < 28) & (pl.col(\"subcarrier_idxs\") > -28))\n",
    "        .group_by(\"index\", maintain_order=True)\n",
    "        .agg([\"subcarrier_idxs\", \"csi_abs\", \"csi_phase\"])\n",
    "        .drop(\"index\")\n",
    "    )\n",
    "\n",
    "    # Overwrite csi abs and subcarrier idxs in original dataframe\n",
    "    return df.with_columns(filtered.get_columns())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600fbdad-9ea2-4145-90bd-d09df3cd59c4",
   "metadata": {},
   "source": [
    "# AGC correction\n",
    "\n",
    "The intel iwl5300 tool yields a special value for AGC correction. Currently, it is the only card to do so. Judging from experiments with precoding curves and visual comparison, however, the iwl5300 is the only card to not include an AGC corrected RSSI.\n",
    "\n",
    "The following method incorporates the iwl5300 AGC value into a corrected RSSI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da775e-87d7-4489-a203-55546956ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agc_correct_rssi(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Use AGC values, if available, to apply RSSI correction.\n",
    "\n",
    "    NOTE: Input must be the original (still wrapped) dataframe\n",
    "    \"\"\"\n",
    "    logger.info(\"Performing AGC RSSI correction ...\")\n",
    "\n",
    "    df = df.join(\n",
    "        df.with_columns(\n",
    "            antenna_rssi_db=10 ** (pl.col(\"antenna_rssi\") / 10),\n",
    "            rssi_db=10 ** (pl.col(\"rssi\") / 10),\n",
    "        )\n",
    "        .group_by([\"capture_num\"], maintain_order=True)\n",
    "        .agg(\n",
    "            pl.col(\"antenna_rssi_db\").sum().alias(\"combined_antenna_rssi\"),\n",
    "            pl.col(\"rssi_db\").sum().alias(\"combined_rssi\"),\n",
    "        ),\n",
    "        on=\"capture_num\",\n",
    "    )\n",
    "\n",
    "    return df.with_columns(\n",
    "        antenna_rssi=pl.when(pl.col(\"agc\").is_null())\n",
    "        .then(pl.col(\"antenna_rssi\"))\n",
    "        .otherwise(\n",
    "            (10 * pl.col(\"combined_antenna_rssi\").log(base=10)) - 44 - pl.col(\"agc\")\n",
    "        ),\n",
    "        rssi=pl.when(pl.col(\"agc\").is_null())\n",
    "        .then(pl.col(\"rssi\"))\n",
    "        .otherwise((10 * pl.col(\"combined_rssi\").log(base=10)) - 44 - pl.col(\"agc\")),\n",
    "    ).drop(\"combined_rssi\", \"combined_antenna_rssi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9697ad9-4c9a-440b-884f-54de6575cda1",
   "metadata": {},
   "source": [
    "# Phase Detrending\n",
    "\n",
    "Sampling Time offsets introduce a random-offset linear-slope phase shift across subcarriers. To rephrase this, the very first phase value on a subcarrier is completely uniformly distributed. To make phases even comparable, we need some phase sanitization procedures.\n",
    "\n",
    "The simplest one is to fix two reference values to get rid of the introduced linear offsets. Specifically, by fixing the outermost subcarriers to have phases $0$, we can linearly detrend them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11227b04-f4ef-4892-ab86-658e9d167283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrend_phase(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # -- Linear phase correction to get rid of STO effect\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    logger.trace(\n",
    "        \"Detrending phase by fitting line through first and last phase values \"\n",
    "        + \"and subtracting that.\"\n",
    "    )\n",
    "\n",
    "    # Columns to use for linear phase normalization\n",
    "    phase_helper_cols = {\n",
    "        \"phase_slope\": (\n",
    "            pl.col(\"unwrapped_phase\").list.last()\n",
    "            - pl.col(\"unwrapped_phase\").list.first()\n",
    "        )\n",
    "        / pl.col(\"subcarrier_idxs\").list.len(),\n",
    "        \"lowest_sc\": pl.col(\"subcarrier_idxs\").list.first(),\n",
    "        \"sc_offset\": pl.col(\"unwrapped_phase\").list.first(),\n",
    "        \"unwrapped_phase\": pl.col(\"unwrapped_phase\"),\n",
    "    }\n",
    "\n",
    "    # Assign row index to allow matching from derived dataframe back to here\n",
    "    df = df.with_row_index(\"phase_row_idx\")\n",
    "\n",
    "    # Aggregate phases and calculate helper columns\n",
    "    phase_helpers = df.with_columns(\n",
    "        unwrapped_phase=pl.col(\"csi_phase\").map_elements(\n",
    "            lambda x: np.unwrap(np.array(x)).tolist()\n",
    "        ),\n",
    "    ).select(\"subcarrier_idxs\", \"phase_row_idx\", **phase_helper_cols)\n",
    "\n",
    "    # To correct linear trend: Shift down by first value, then remove linear slope\n",
    "    # in dependence of subcarrier number\n",
    "    lin_corr_expr = (\n",
    "        pl.col(\"unwrapped_phase\")\n",
    "        - pl.col(\"sc_offset\")\n",
    "        - ((pl.col(\"subcarrier_idxs\") - pl.col(\"lowest_sc\")) * pl.col(\"phase_slope\"))\n",
    "    )\n",
    "\n",
    "    def phase_sanitizer(unwrapped_df):\n",
    "        return unwrapped_df.with_columns(normed_csi_phase=lin_corr_expr)\n",
    "\n",
    "    # Apply phase sanitization on unwrapped dataframe\n",
    "    sanitized_phases = on_unwrapped(\n",
    "        phase_helpers, phase_sanitizer, [\"unwrapped_phase\", \"subcarrier_idxs\"]\n",
    "    )\n",
    "\n",
    "    # Match back\n",
    "    return df.update(sanitized_phases, on=\"phase_row_idx\").drop(\"phase_row_idx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31b655-b6de-4317-a69c-cae271eff953",
   "metadata": {},
   "source": [
    "# Subselection\n",
    "\n",
    "Sharing of gigabyte sized databases is not always needed. Sometimes we just want to take the first $n$ of something, e.g. the first $5$ sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08071a19-e782-4ad5-ae16-6a9af84f2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_n_curves(df: pl.DataFrame, num_curves: int) -> pl.DataFrame:\n",
    "    logger.info(f\"Extracting data of the first {num_curves} curves present.\")\n",
    "    first_n = (\n",
    "        df.select(\"num_curve\").unique(\"num_curve\", maintain_order=True).head(num_curves)\n",
    "    )\n",
    "    return df.join(first_n, on=\"num_curve\")\n",
    "\n",
    "\n",
    "def first_n_sessions(df: pl.DataFrame, num_sessions: int) -> pl.DataFrame:\n",
    "    logger.info(f\"Extracting first {num_sessions} sessions for each curve\")\n",
    "\n",
    "    def first_n_filter(group: pl.DataFrame) -> pl.DataFrame:\n",
    "        keys = (\n",
    "            group.select(\"session_id\")\n",
    "            .unique(\"session_id\", maintain_order=True)\n",
    "            .head(num_sessions)\n",
    "        )\n",
    "        test = group.join(keys, on=\"session_id\", how=\"semi\")\n",
    "        return test\n",
    "\n",
    "    return df.group_by(\"num_curve\", \"receiver_name\", maintain_order=True).map_groups(\n",
    "        first_n_filter\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b2206-a16b-49ff-8069-80a66f1d85e3",
   "metadata": {},
   "source": [
    "# Data shape alignment\n",
    "\n",
    "One of the consequences of cards dropping frames is that unequal shapes of DataFrames/arrays occur.\n",
    "\n",
    "Consider for example a session with $1000$ packets sent. If two cards drop $10$ and $100$ values, respectively, the resulting\n",
    "shapes for those sessions would be $(990, n_{subcs})$ and $(900, n_{subcs})$. If we want to join these in a numpy array, we will run into errors.\n",
    "\n",
    "Similar to the sequence number alignment above, we can use them to figure out which values are missing and simply input Nulls. Follow-up routines can then decode whether to get rid of nulls by subsampling or maybe performing interpolation/imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e695a-e6ed-4c34-9505-ec9a0672b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_nulls(df: pl.DataFrame, sequence_len: int) -> pl.DataFrame:\n",
    "    logger.info(\"Inserting Nulls for missing sequence number values\")\n",
    "\n",
    "    complete_index = pl.DataFrame(\n",
    "        {\"sequence_number\": range(0, sequence_len)},\n",
    "        schema={\"sequence_number\": pl.UInt16},\n",
    "    )\n",
    "\n",
    "    # NOTE: Inserting nulls inserts null in ALL columns. The metadata columns are of course\n",
    "    # known even there. For that, we need to fill them back in immediately.\n",
    "    def null_inserter(group):\n",
    "        if group.item(0, \"mask_name\") == \"_unmodified\":\n",
    "            group = group.select(\"sequence_number\", pl.exclude(\"sequence_number\"))\n",
    "            return group\n",
    "\n",
    "        n_subcs = len(group.item(0, \"subcarrier_idxs\"))\n",
    "        cgroup = complete_index.join(group, on=\"sequence_number\", how=\"left\")\n",
    "\n",
    "        cgroup = (\n",
    "            cgroup.with_columns(\n",
    "                cgroup[\"session_id\"]\n",
    "                .fill_null(strategy=\"forward\")\n",
    "                .fill_null(strategy=\"backward\")\n",
    "            )\n",
    "            .with_columns(\n",
    "                cgroup[\"num_curve\"]\n",
    "                .fill_null(strategy=\"forward\")\n",
    "                .fill_null(strategy=\"backward\")\n",
    "            )\n",
    "            .with_columns(\n",
    "                cgroup[\"receiver_name\"]\n",
    "                .fill_null(strategy=\"forward\")\n",
    "                .fill_null(strategy=\"backward\")\n",
    "            )\n",
    "            .with_columns(\n",
    "                cgroup[\"subcarrier_idxs\"]\n",
    "                .fill_null(strategy=\"forward\")\n",
    "                .fill_null(strategy=\"backward\")\n",
    "            )\n",
    "            .with_columns(\n",
    "                cgroup[\"mask_name\"]\n",
    "                .fill_null(strategy=\"forward\")\n",
    "                .fill_null(strategy=\"backward\")\n",
    "            )\n",
    "            .with_columns(cgroup[\"csi_abs\"].fill_null(value=[None] * n_subcs))\n",
    "            .with_columns(cgroup[\"csi_phase\"].fill_null(value=[None] * n_subcs))\n",
    "        )\n",
    "\n",
    "        return cgroup\n",
    "\n",
    "    # Fill!\n",
    "    df = df.group_by(\n",
    "        \"session_id\", \"receiver_name\", \"mask_name\", maintain_order=True\n",
    "    ).map_groups(null_inserter)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f24bf3-4572-448a-96d3-2a463853ae20",
   "metadata": {},
   "source": [
    "# Linear interpolation / imputation\n",
    "\n",
    "The simplest method, which can be done in polars natively, is interpolation by linear values. Note that this doesn't work at the edges, it lineraly interpolates between two values only, not on larger subsets. On the edges, we therefore simply extend the last valid value.\n",
    "\n",
    "This method is only suitable for very clean datasets in which only few values at a time are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a6081-fef0-42ed-9a92-6d9997820bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Interpolate missing (Null) using a linear fit and constant extensions at the edges\n",
    "\n",
    "    TODO: Implement further methods, such as Gaussian Process Regression:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor\n",
    "    \"\"\"\n",
    "    logger.info(\"Interpolating missing (Null) values\")\n",
    "\n",
    "    def interpolator(unwrapped_df):\n",
    "        return unwrapped_df.group_by(\n",
    "            \"subcarrier_idxs\", \"receiver_name\", \"session_id\", maintain_order=True\n",
    "        ).map_groups(\n",
    "            lambda group: group.with_columns(\n",
    "                group.select(\"timestamp\", \"csi_abs\", \"csi_phase\", \"antenna_rssi\")\n",
    "                .interpolate()\n",
    "                .fill_null(strategy=\"forward\")\n",
    "                .fill_null(strategy=\"backward\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "    nested_cols = [\"csi_abs\", \"csi_phase\", \"subcarrier_idxs\"]\n",
    "    return on_unwrapped(df, interpolator, nested_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ade84f-0e78-4bd7-942f-04a9cf8f059d",
   "metadata": {},
   "source": [
    "# Assembly of preprocessing\n",
    "\n",
    "Finally, let's assemble all the preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf6e0f-240d-4373-ac56-b5e573e5a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PreprocessingConfig:\n",
    "    discard_low_counts: bool = False\n",
    "    discard_unmodified: bool = False\n",
    "    align_sequence_nums: bool = False\n",
    "    remove_edge_subcs: bool = True\n",
    "    merge_session_pairs: bool = True\n",
    "    correct_agc: bool = True\n",
    "    squash_antennas: bool = True  # Only use when just one antenna used!\n",
    "    insert_nulls: bool = True\n",
    "    impute_missing: bool = True\n",
    "    detrend_phase: bool = False\n",
    "    sequence_len: int = 4000\n",
    "    low_count_threshold = 1900\n",
    "    num_curves = 20\n",
    "    num_sessions = 10\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Preprocess. The dataframe contains a lot of duplicate junk data.\n",
    "# I need to rewrite that part for deduplication, but didnt take the\n",
    "# time so far. Therefore, I just discard the junk. You can ignore this,\n",
    "# the dataframe shared with you will already have those columns dropped\n",
    "#\n",
    "# This does:\n",
    "# - Perform corrections\n",
    "# - Remove captures of insufficient size\n",
    "# - Remove curves with insufficient captures\n",
    "# - Balance out data (since ax210, asus are overrepresented)\n",
    "# - Align sequence numbers\n",
    "# - Insert \"missing\" data with Nulls (based on missing sequence numbers)\n",
    "# - Remove unnecessary columns\n",
    "def preprocess(df: pl.DataFrame, config: PreprocessingConfig):\n",
    "    unused_columns = [\n",
    "        \"experiment_start_time\",\n",
    "        \"mask_id\",\n",
    "        \"bandwidth\",\n",
    "        \"tx_gain\",\n",
    "        \"channel\",\n",
    "        \"session_timestamp\",\n",
    "        \"frame_id\",\n",
    "        \"experiment_id\",\n",
    "        \"session_name\",\n",
    "        \"n_receivers\",\n",
    "    ]\n",
    "\n",
    "    if config.discard_unmodified:\n",
    "        df = discard_unmodified(df)\n",
    "        unused_columns.append(\"mask_name\")\n",
    "\n",
    "    if config.align_sequence_nums:\n",
    "        df = align_sequence_nums(df)\n",
    "\n",
    "    # NOTE: csi_abs and csi_phase are nested for each of the receive antennas\n",
    "    # Here we unrap this nesting into separate CSI per antenna\n",
    "    df = df.with_row_index(\"capture_num\").explode(\n",
    "        \"used_antenna_idxs\", \"csi_abs\", \"csi_phase\", \"antenna_rssi\"\n",
    "    )\n",
    "\n",
    "    # We store labels in the session name. Extract them into separate columns\n",
    "    df = df.with_columns(\n",
    "        num_curve=pl.col(\"session_name\")\n",
    "        .str.extract(\"^.*shape_([0-9]+)\")\n",
    "        .cast(pl.UInt8),\n",
    "    )\n",
    "\n",
    "    df = df.drop(unused_columns)\n",
    "    df = df.collect()\n",
    "\n",
    "    # Either merge pairs of iwl/qca captures, or treat as wanting double the\n",
    "    # amount of sessions\n",
    "    if config.merge_session_pairs:\n",
    "        df = merge_session_pairs(df)\n",
    "\n",
    "    # Discarding low counts and then curves with insufficient session numbers\n",
    "    if config.discard_low_counts:\n",
    "        df = discard_low_counts(df, config.low_count_threshold, grouped=False)\n",
    "        df = discard_low_session_counts(df, config.num_sessions)\n",
    "\n",
    "    # Extracting first n sessions and curves\n",
    "    df = first_n_sessions(df, config.num_sessions)\n",
    "    df = first_n_curves(df, config.num_curves)\n",
    "\n",
    "    if config.remove_edge_subcs:\n",
    "        df = remove_edge_subcs(df)\n",
    "\n",
    "    if config.correct_agc:\n",
    "        df = agc_correct_rssi(df)\n",
    "        df = df.drop(\"agc\")\n",
    "\n",
    "    if config.squash_antennas:\n",
    "        df = df.drop(\"rssi\", \"used_antenna_idxs\")\n",
    "\n",
    "    if config.insert_nulls:\n",
    "        df = insert_nulls(df, config.sequence_len)\n",
    "\n",
    "    if config.impute_missing:\n",
    "        df = interpolate(df)\n",
    "\n",
    "    if config.detrend_phase:\n",
    "        df = detrend_phase(df)\n",
    "\n",
    "    # Drop temporary capture num row index\n",
    "    df = df.drop(\"capture_num\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb4964-69f5-41ae-88e3-d9dcd4261b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary file to store processed parquet in\n",
    "exp_dir = Path.cwd().parent / \"data\" / \"random_dyncurves_diffspeeds\"\n",
    "tmp_db_file = exp_dir / \"damped_preprocessed.parquet\"\n",
    "\n",
    "# Preprocess from data directory if not done yet\n",
    "df = pl.scan_parquet(exp_dir / \"damped.parquet\")\n",
    "\n",
    "config = PreprocessingConfig()\n",
    "df = preprocess(df, config)\n",
    "\n",
    "# NOTE: 700 CSI per session, 5 receiver, 10 curves and 20 sessions = 700_000 rows\n",
    "logger.info(f\"FINISHED PREPROCESSING. Shape of preprocessed dataframe: {df.shape}\")\n",
    "logger.info(f\"Number of sessions in dataframe: {df.n_unique('session_id')}\")\n",
    "logger.info(f\"Number of curves in dataframe: {df.n_unique('num_curve')}\")\n",
    "print(df.columns)\n",
    "\n",
    "df.write_parquet(tmp_db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273fd7c1-4fe5-4154-9504-d365ce1465bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
